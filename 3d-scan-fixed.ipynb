{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "runner-cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from pathlib import Path\n",
                "\n",
                "# ================= CONFIGURATION =================\n",
                "# ‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ï‡πà‡∏≠‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏° ‡πÉ‡∏´‡πâ‡πÉ‡∏™‡πà Path ‡∏Ç‡∏≠‡∏á Dataset ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà\n",
                "# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: \"/kaggle/input/my-dataset/car_scan\"\n",
                "RESUME_PATH = \"\"  # Leave empty if starting new\n",
                "# =================================================\n",
                "\n",
                "print(\"üßπ 1. ‡πÄ‡∏Ñ‡∏•‡∏µ‡∏¢‡∏£‡πå‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏Ñ‡πâ‡∏î‡πÉ‡∏´‡∏°‡πà...\")\n",
                "if os.path.exists(\"3DSCAN\"):\n",
                "    !rm -rf 3DSCAN\n",
                "\n",
                "# Clone repo (optional if we overwrite anyway, but good for other files)\n",
                "!git clone https://github.com/PRIDA-TAKON/3DSCAN.git\n",
                "os.chdir(\"3DSCAN\")\n",
                "\n",
                "# --- OVERWRITE 3d-scan.py WITH FIX ---\n",
                "fixed_script = r'''import os\n",
                "import shutil\n",
                "import sys\n",
                "import glob\n",
                "import subprocess\n",
                "import argparse\n",
                "from pathlib import Path\n",
                "import json\n",
                "import importlib.util\n",
                "import struct\n",
                "import math\n",
                "import numpy as np\n",
                "\n",
                "print(\"‚úÖ Imports complete\")\n",
                "\n",
                "# ================= CONFIGURATION =================\n",
                "PROJECT_NAME = \"car_scan\"\n",
                "# IMPORTANT: Update this path to match your uploaded video in Kaggle\n",
                "VIDEO_INPUT_PATH = Path('/kaggle/input/car-video/video_car.mp4')\n",
                "if not VIDEO_INPUT_PATH.exists() and Path(\"input/video_car.mp4\").exists():\n",
                "    VIDEO_INPUT_PATH = Path(\"input/video_car.mp4\")\n",
                "\n",
                "# Allow overriding via command line\n",
                "import argparse\n",
                "# We need to parse args early to set the constant, or move this logic inside main. \n",
                "# But this script uses global constants. Let's look for args.\n",
                "parser_pre = argparse.ArgumentParser(add_help=False)\n",
                "parser_pre.add_argument(\"--input_video\", type=str, default=None)\n",
                "args_pre, _ = parser_pre.parse_known_args()\n",
                "if args_pre.input_video:\n",
                "    VIDEO_INPUT_PATH = Path(args_pre.input_video)\n",
                "\n",
                "WORKING_DIR = Path(\"/kaggle/working\")\n",
                "if not WORKING_DIR.exists():\n",
                "    WORKING_DIR = Path.cwd() / \"working_data\"\n",
                "    WORKING_DIR.mkdir(parents=True, exist_ok=True)\n",
                "PROJECT_DIR = WORKING_DIR / PROJECT_NAME\n",
                "DATABASE_PATH = PROJECT_DIR / \"database.db\"\n",
                "IMAGES_DIR = PROJECT_DIR / \"images\"\n",
                "SPARSE_PATH = PROJECT_DIR / \"sparse\"\n",
                "OUTPUTS_DIR = Path(\"outputs\") / PROJECT_NAME / \"splatfacto\"\n",
                "\n",
                "# Environment tweaks\n",
                "os.environ['MAX_JOBS'] = '1' # Prevent freezing on Kaggle\n",
                "\n",
                "def run_command(cmd, shell=False):\n",
                "    \"\"\"Runs a shell command and raises an exception if it fails.\"\"\"\n",
                "    print(f\"üöÄ Running: {cmd}\")\n",
                "    try:\n",
                "        if shell:\n",
                "            subprocess.run(cmd, shell=True, check=True)\n",
                "        else:\n",
                "            if isinstance(cmd, str) and not shell:\n",
                "                cmd = cmd.split()\n",
                "            subprocess.run(cmd, check=True)\n",
                "    except subprocess.CalledProcessError as e:\n",
                "        print(f\"‚ùå Command failed: {cmd}\")\n",
                "        raise e\n",
                "\n",
                "def check_gpu():\n",
                "    print(\"üîç Checking GPU availability...\")\n",
                "    try:\n",
                "        import torch\n",
                "        if not torch.cuda.is_available():\n",
                "            print(\"‚ö†Ô∏è\" * 20)\n",
                "            print(\"‚ö†Ô∏è WARNING: GPU Not Detected!\")\n",
                "            print(\"‚ö†Ô∏è This script requires a GPU (P100 or T4) to run effectively.\")\n",
                "            print(\"‚ö†Ô∏è Please enable GPU Accelerator in your Kaggle Notebook settings.\")\n",
                "            print(\"‚ö†Ô∏è\" * 20)\n",
                "            return False\n",
                "        print(f\"‚úÖ GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
                "        return True\n",
                "    except ImportError:\n",
                "        print(\"‚ö†Ô∏è torch module not found. Cannot check GPU availability.\")\n",
                "        return False\n",
                "\n",
                "def install_dependencies():\n",
                "    print(\"‚è≥ Installing dependencies...\")\n",
                "\n",
                "    # Check if nerfstudio is installed\n",
                "    if importlib.util.find_spec(\"nerfstudio\") is None:\n",
                "        run_command(\"pip install --upgrade pip\", shell=True)\n",
                "        # Force numpy < 2.0 to avoid compatibility issues with recent library updates\n",
                "        # \"Factory Reset\" numpy: force reinstall to fix potential file corruption from previous patching attempts\n",
                "        run_command(\"pip install \\\"numpy<2.0\\\" --force-reinstall\", shell=True)\n",
                "        run_command(\"pip install torch torchvision\", shell=True)\n",
                "        run_command(\"pip install nerfstudio\", shell=True)\n",
                "        run_command(\"pip install plyfile\", shell=True)\n",
                "    else:\n",
                "        print(\"   nerfstudio already installed.\")\n",
                "\n",
                "    print(\"‚è≥ Installing COLMAP & ffmpeg...\")\n",
                "    run_command(\"apt-get update\", shell=True)\n",
                "\n",
                "    # Check if colmap is installed\n",
                "    try:\n",
                "        run_command(\"colmap help\", shell=True)\n",
                "        print(\"   COLMAP already installed.\")\n",
                "    except:\n",
                "        print(\"‚è≥ Installing COLMAP via apt-get...\")\n",
                "        run_command(\"apt-get install -y colmap\", shell=True)\n",
                "\n",
                "    # Check if ffmpeg is installed\n",
                "    try:\n",
                "        run_command(\"ffmpeg -version\", shell=True)\n",
                "        print(\"   ffmpeg already installed.\")\n",
                "    except:\n",
                "        run_command(\"apt-get install -y ffmpeg\", shell=True)\n",
                "\n",
                "    # Check if xvfb is installed (required for COLMAP with GPU)\n",
                "    try:\n",
                "        run_command(\"which xvfb-run\", shell=True)\n",
                "        print(\"   xvfb already installed.\")\n",
                "    except:\n",
                "        print(\"‚è≥ Installing xvfb...\")\n",
                "        run_command(\"apt-get install -y xvfb\", shell=True)\n",
                "\n",
                "    try:\n",
                "        run_command(\"colmap help\", shell=True)\n",
                "        print(\"‚úÖ COLMAP installed successfully.\")\n",
                "    except:\n",
                "        print(\"‚ùå COLMAP installation failed.\")\n",
                "\n",
                "\n",
                "\n",
                "def patch_nerfstudio():\n",
                "    \"\"\"\n",
                "    Patches nerfstudio installed in the system to fix PyTorch 2.6+ compatibility issues.\n",
                "    \"\"\"\n",
                "    print(\"üîß Patching nerfstudio for PyTorch 2.6+ compatibility...\")\n",
                "    try:\n",
                "        potential_paths = glob.glob(\"/usr/local/lib/python*/dist-packages/nerfstudio/utils/eval_utils.py\")\n",
                "        if not potential_paths:\n",
                "            potential_paths = glob.glob(\"/opt/conda/lib/python*/site-packages/nerfstudio/utils/eval_utils.py\")\n",
                "\n",
                "        if potential_paths:\n",
                "            target_file = Path(potential_paths[0])\n",
                "            print(f\"   Found file: {target_file}\")\n",
                "\n",
                "            with open(target_file, \"r\") as f:\n",
                "                content = f.read()\n",
                "\n",
                "            old_code = 'loaded_state = torch.load(load_path, map_location=\"cpu\")'\n",
                "            new_code = 'loaded_state = torch.load(load_path, map_location=\"cpu\", weights_only=False)'\n",
                "\n",
                "            if old_code in content:\n",
                "                new_content = content.replace(old_code, new_code)\n",
                "                with open(target_file, \"w\") as f:\n",
                "                    f.write(new_content)\n",
                "                print(\"‚úÖ Patch applied successfully!\")\n",
                "            elif 'weights_only=False' in content:\n",
                "                 print(\"‚úÖ Patch was already applied.\")\n",
                "            else:\n",
                "                print(f\"‚ö†Ô∏è Target code not found in {target_file}. The library version might be different.\")\n",
                "        else:\n",
                "            print(\"‚ö†Ô∏è Could not locate nerfstudio/utils/eval_utils.py to patch.\")\n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Failed to patch nerfstudio: {e}\")\n",
                "\n",
                "def process_data(resume_path=None):\n",
                "    \"\"\"\n",
                "    Processes video into images and run COLMAP, OR resumes from existing data.\n",
                "    \"\"\"\n",
                "    if resume_path:\n",
                "        print(f\"üîÑ RESUME MODE ENABLED. Loading data from: {resume_path}\")\n",
                "        resume_source = Path(resume_path)\n",
                "        \n",
                "        if not resume_source.exists():\n",
                "            print(f\"‚ùå Error: Resume path not found at {resume_source}\")\n",
                "            return False\n",
                "\n",
                "        # Create project directory if it doesn't exist\n",
                "        PROJECT_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "        # List of critical items to copy\n",
                "        items_to_copy = [\"transforms.json\", \"images\", \"sparse\", \"database.db\", \"sparse_pc.ply\"]\n",
                "        \n",
                "        for item in items_to_copy:\n",
                "            src = resume_source / item\n",
                "            dst = PROJECT_DIR / item\n",
                "            \n",
                "            if src.exists():\n",
                "                if dst.exists():\n",
                "                    print(f\"   Removing existing {dst}...\")\n",
                "                    if dst.is_dir():\n",
                "                        shutil.rmtree(dst)\n",
                "                    else:\n",
                "                        dst.unlink()\n",
                "                \n",
                "                print(f\"   Copying {item}...\")\n",
                "                if src.is_dir():\n",
                "                    shutil.copytree(src, dst)\n",
                "                else:\n",
                "                    shutil.copy2(src, dst)\n",
                "            else:\n",
                "                 print(f\"‚ö†Ô∏è Warning: '{item}' not found in resume source. Proceeding cautiously.\")\n",
                "\n",
                "        if (PROJECT_DIR / \"transforms.json\").exists():\n",
                "            print(\"‚úÖ Data restored successfully via Resume.\")\n",
                "            return True\n",
                "        else:\n",
                "             print(\"‚ùå Failed to restore 'transforms.json'. Resume invalid.\")\n",
                "             return False\n",
                "\n",
                "    # --- NORMAL PROCESSING START ---\n",
                "    if not VIDEO_INPUT_PATH.exists():\n",
                "        print(f\"‚ùå Error: Video file not found at {VIDEO_INPUT_PATH}\")\n",
                "        print(\"Please upload your video and update VIDEO_INPUT_PATH in the script.\")\n",
                "        return False\n",
                "\n",
                "    print(\"--- 1. Clean & Setup ---\")\n",
                "    if PROJECT_DIR.exists():\n",
                "        shutil.rmtree(PROJECT_DIR)\n",
                "    PROJECT_DIR.mkdir(parents=True, exist_ok=True)\n",
                "    IMAGES_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "    # Determine COLMAP binary command (use xvfb-run if available)\n",
                "    colmap_binary = \"colmap\"\n",
                "    try:\n",
                "        run_command(\"which xvfb-run\", shell=True)\n",
                "        colmap_binary = \"xvfb-run -a colmap\"\n",
                "        print(f\"‚úÖ xvfb-run detected. Using: {colmap_binary}\")\n",
                "    except:\n",
                "        print(\"‚ö†Ô∏è xvfb-run not found. Using raw colmap command.\")\n",
                "\n",
                "    print(\"--- 2. Downscale Video ---\")\n",
                "    downscaled_video = WORKING_DIR / f\"{PROJECT_NAME}_downscaled.mp4\"\n",
                "    # Remove audio (-an) to prevent sync issues causing truncation\n",
                "    # Use -vf scale=iw/2:ih/2 (no quotes) which works on both Linux/Windows usually if no spaces\n",
                "    run_command(f\"ffmpeg -y -i \\\"{VIDEO_INPUT_PATH}\\\" -vf scale=iw/2:ih/2 -c:v libx264 -preset veryfast -crf 23 -an \\\"{downscaled_video}\\\"\", shell=True)\n",
                "\n",
                "    print(\"--- 3. Extract Frames (2 FPS) ---\")\n",
                "    # Remove -loglevel error to see progress/errors\n",
                "    run_command(f\"ffmpeg -y -i \\\"{downscaled_video}\\\" -vf \\\"fps=2\\\" \\\"{IMAGES_DIR}/frame_%05d.png\\\" -hide_banner\", shell=True)\n",
                "\n",
                "    num_images = sum(1 for _ in os.scandir(IMAGES_DIR))\n",
                "    print(f\"‚úÖ Extracted {num_images} images.\")\n",
                "\n",
                "    print(\"--- 4. Feature Extraction ---\")\n",
                "    # Using CPU for feature extraction as per original notebook config, but memory says SiftMatching should use GPU.\n",
                "    # Feature extraction is separate from Matching. Memory specifically says SiftMatching.\n",
                "    # However, usually if one uses GPU, the other can too.\n",
                "    # The notebook says: --SiftExtraction.use_gpu 0\n",
                "    # Memory says: \"COLMAP SIFT matching and extraction commands in the project should have GPU acceleration enabled (`use_gpu 1`) to maximize processing speed.\"\n",
                "    # So I should enable GPU for extraction too.\n",
                "\n",
                "    cmd_extract = [\n",
                "        colmap_binary, \"feature_extractor\",\n",
                "        \"--database_path\", str(DATABASE_PATH),\n",
                "        \"--image_path\", str(IMAGES_DIR),\n",
                "        \"--ImageReader.camera_model\", \"OPENCV\",\n",
                "        \"--SiftExtraction.use_gpu\", \"0\", # Disable GPU for extraction to avoid OpenGL crashes in headless mode\n",
                "        \"--SiftExtraction.num_threads\", \"16\",\n",
                "        \"--SiftExtraction.peak_threshold\", \"0.004\",\n",
                "    ]\n",
                "    run_command(\" \".join(cmd_extract), shell=True)\n",
                "\n",
                "    print(\"--- 5. Matching (Sequential) ---\")\n",
                "    # --- FIX 2: Disable loop_detection to avoid crash due to missing vocab tree ---\n",
                "    cmd_match = [\n",
                "        colmap_binary, \"sequential_matcher\",\n",
                "        \"--database_path\", str(DATABASE_PATH),\n",
                "        \"--SiftMatching.use_gpu\", \"0\",\n",
                "        \"--SequentialMatching.loop_detection\", \"0\",\n",
                "        \"--SequentialMatching.overlap\", \"10\"\n",
                "    ]\n",
                "    run_command(\" \".join(cmd_match), shell=True)\n",
                "\n",
                "    print(\"--- 6. Mapper (Relaxed) ---\")\n",
                "    SPARSE_PATH.mkdir(parents=True, exist_ok=True)\n",
                "    cmd_mapper = [\n",
                "        colmap_binary, \"mapper\",\n",
                "        \"--database_path\", str(DATABASE_PATH),\n",
                "        \"--image_path\", str(IMAGES_DIR),\n",
                "        \"--output_path\", str(SPARSE_PATH),\n",
                "        \"--Mapper.min_num_matches\", \"10\",\n",
                "        \"--Mapper.init_min_tri_angle\", \"2\",\n",
                "        \"--Mapper.multiple_models\", \"0\"\n",
                "    ]\n",
                "    run_command(\" \".join(cmd_mapper), shell=True)\n",
                "\n",
                "    print(\"--- 7. Converting to transforms.json ---\")\n",
                "    recon_dir = SPARSE_PATH / \"0\"\n",
                "    if not recon_dir.exists():\n",
                "        print(\"‚ùå FAILED: Sparse reconstruction failed. No model found.\")\n",
                "        return False\n",
                "\n",
                "    from nerfstudio.process_data.colmap_utils import colmap_to_json\n",
                "    colmap_to_json(\n",
                "        recon_dir=recon_dir,\n",
                "        output_dir=PROJECT_DIR,\n",
                "    )\n",
                "\n",
                "    if (PROJECT_DIR / \"transforms.json\").exists():\n",
                "        print(\"‚úÖ transforms.json created.\")\n",
                "        return True\n",
                "    else:\n",
                "        print(\"‚ùå Failed to create transforms.json\")\n",
                "        return False\n",
                "\n",
                "def train_model():\n",
                "    print(\"--- Training Splatfacto Model ---\")\n",
                "    # ns-train splatfacto --data {PROJECT_DIR} --viewer.quit-on-train-completion True\n",
                "    cmd_train = f\"ns-train splatfacto --data \\\"{PROJECT_DIR}\\\" --viewer.quit-on-train-completion True\"\n",
                "    run_command(cmd_train, shell=True)\n",
                "\n",
                "def convert_ply_to_splat(ply_file: Path, output_file: Path):\n",
                "    \"\"\"\n",
                "    Converts a PLY file to a .splat file.\n",
                "    \"\"\"\n",
                "    print(f\"‚è≥ Converting {ply_file.name} to .splat format...\")\n",
                "    from plyfile import PlyData\n",
                "    import numpy as np\n",
                "    \n",
                "    try:\n",
                "        plydata = PlyData.read(str(ply_file))\n",
                "        vert = plydata[\"vertex\"]\n",
                "        \n",
                "        # Use sorting to improve rendering order (closest first is usually handled by viewer sorting, \n",
                "        # but splat files are often sorted by morton code or similar. Here we just pack data).\n",
                "        # Some viewers expect sorting. For simple purposes, we just pack.\n",
                "        \n",
                "        sorted_indices = np.argsort(\n",
                "            -np.exp(vert[\"scale_0\"] + vert[\"scale_1\"] + vert[\"scale_2\"])\n",
                "            / (1 / (1 + np.exp(-vert[\"opacity\"])))\n",
                "        )\n",
                "        \n",
                "        buffer = bytearray()\n",
                "        for idx in sorted_indices:\n",
                "            position = np.array([vert[\"x\"][idx], vert[\"y\"][idx], vert[\"z\"][idx]], dtype=np.float32)\n",
                "            scales = np.array([vert[\"scale_0\"][idx], vert[\"scale_1\"][idx], vert[\"scale_2\"][idx]], dtype=np.float32)\n",
                "            rot = np.array([vert[\"rot_0\"][idx], vert[\"rot_1\"][idx], vert[\"rot_2\"][idx], vert[\"rot_3\"][idx]], dtype=np.float32)\n",
                "            \n",
                "            # Color (Spherical Harmonics DC term)\n",
                "            # SH_0(0), SH_0(1), SH_0(2) corresponds to R, G, B DC components\n",
                "            # Usually in Ply from Nerfstudio it's f_dc_0, f_dc_1, f_dc_2\n",
                "            SH_C0 = 0.28209479177387814\n",
                "            r = max(0, min(255, int((0.5 + SH_C0 * vert[\"f_dc_0\"][idx]) * 255)))\n",
                "            g = max(0, min(255, int((0.5 + SH_C0 * vert[\"f_dc_1\"][idx]) * 255)))\n",
                "            b = max(0, min(255, int((0.5 + SH_C0 * vert[\"f_dc_2\"][idx]) * 255)))\n",
                "            color = np.array([r, g, b, 255], dtype=np.uint8)\n",
                "\n",
                "            # Normalize Rotation\n",
                "            length = np.sqrt(np.sum(rot ** 2))\n",
                "            rot /= length\n",
                "            \n",
                "            # Exp scales to get linear scale\n",
                "            scales = np.exp(scales)\n",
                "            \n",
                "            # Pack into buffer\n",
                "            # Format: position(3f), scale(3f), color(4b), rotation(4b)\n",
                "            # Note: .splat format spec varies, standard is usually pos, scale, color, rot_q\n",
                "            buffer.extend(position.tobytes())\n",
                "            buffer.extend(scales.tobytes())\n",
                "            buffer.extend(color.tobytes())\n",
                "            \n",
                "            # Quantize Rotation to 8-bit\n",
                "            # rot_int = (rot * 127.5 + 127.5).astype(np.uint8)\n",
                "            # buffer.extend(rot_int.tobytes())\n",
                "            \n",
                "            # Wait, the Standard Gaussian Splatting .splat file format (Antimatter15) is:\n",
                "            # Position (3 floats), Scale (3 floats), Color (4 uint8: R,G,B,A), Rotation (4 uint8: quaternion)\n",
                "            \n",
                "            rot_int = ((rot * 128 + 128).clip(0, 255)).astype(np.uint8)\n",
                "            buffer.extend(rot_int.tobytes())\n",
                "            \n",
                "        with open(output_file, \"wb\") as f:\n",
                "            f.write(buffer)\n",
                "            \n",
                "        print(f\"‚úÖ Successfully converted to {output_file}\")\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Conversion failed: {e}\")\n",
                "\n",
                "def export_model():\n",
                "    print(\"--- Exporting .splat ---\")\n",
                "    training_output_path = OUTPUTS_DIR\n",
                "\n",
                "    if not training_output_path.exists():\n",
                "        print(f\"‚ùå Error: Training output directory not found at {training_output_path}\")\n",
                "        return\n",
                "\n",
                "    latest_run = None\n",
                "    latest_mtime = -1\n",
                "\n",
                "    with os.scandir(training_output_path) as it:\n",
                "        for entry in it:\n",
                "            if entry.is_dir():\n",
                "                if entry.stat().st_mtime > latest_mtime:\n",
                "                    latest_mtime = entry.stat().st_mtime\n",
                "                    latest_run = Path(entry.path)\n",
                "\n",
                "    if latest_run is None:\n",
                "         print(\"‚ùå Error: No training run folders found.\")\n",
                "         return\n",
                "\n",
                "    config_path = latest_run / \"config.yml\"\n",
                "\n",
                "    if not config_path.exists():\n",
                "        print(f\"‚ùå Error: Config file not found in {latest_run}\")\n",
                "        return\n",
                "\n",
                "    print(f\"‚úÖ Found latest config: {config_path}\")\n",
                "\n",
                "    # Run export (Standard PLY)\n",
                "    cmd_export = f\"ns-export gaussian-splat --load-config \\\"{config_path}\\\" --output-dir \\\"{latest_run}\\\"\"\n",
                "    run_command(cmd_export, shell=True)\n",
                "\n",
                "    # Verify result\n",
                "    generated_plys = list(latest_run.glob(\"*.ply\"))\n",
                "    if generated_plys:\n",
                "        ply_file = generated_plys[0]\n",
                "        print(f\"üéâ Created PLY file: {ply_file}\")\n",
                "        \n",
                "        # Convert to .splat\n",
                "        splat_file = latest_run / \"model.splat\"\n",
                "        convert_ply_to_splat(ply_file, splat_file)\n",
                "    else:\n",
                "        print(f\"‚ùå Export command finished but no .ply file was found in {latest_run}\")\n",
                "        print(\"üìÇ Directory content:\")\n",
                "        for f in latest_run.iterdir():\n",
                "            print(f\" - {f.name}\")\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    parser = argparse.ArgumentParser(description=\"Run 3D Scan Pipeline\")\n",
                "    parser.add_argument(\"--resume_path\", type=str, help=\"Path to existing project folder (containing transforms.json) to resume from\", default=None)\n",
                "    args = parser.parse_args()\n",
                "\n",
                "    # 1. GPU Check\n",
                "    if not check_gpu():\n",
                "        print(\"WARNING: Proceeding without GPU might fail or be extremely slow.\")\n",
                "\n",
                "    # 2. Install Deps\n",
                "    install_dependencies()\n",
                "\n",
                "    # 3. Apply Patch (Critical Fix)\n",
                "    patch_nerfstudio()\n",
                "\n",
                "    # 4. Process Data (or Resume)\n",
                "    if process_data(resume_path=args.resume_path):\n",
                "        print(\"‚úÖ Data ready.\")\n",
                "\n",
                "        # 5. Train\n",
                "        # Only run if transforms.json exists\n",
                "        if (PROJECT_DIR / \"transforms.json\").exists():\n",
                "            train_model()\n",
                "\n",
                "            # 6. Export\n",
                "            if OUTPUTS_DIR.exists():\n",
                "                export_model()\n",
                "            else:\n",
                "                print(\"‚ùå Skipping export because output directory not found.\")\n",
                "        else:\n",
                "            print(\"‚ùå Skipping training because transforms.json was not found.\")\n",
                "    else:\n",
                "        print(\"‚ùå Data processing failed.\")\n",
                "'''\n",
                "\n",
                "with open(\"3d-scan.py\", \"w\") as f:\n",
                "    f.write(fixed_script)\n",
                "    \n",
                "print(\"‚úÖ Fixed 3DSCAN/3d-scan.py with -an (audio removed) AND .splat conversion support.\")\n",
                "\n",
                "print(f\"\\nüöÄ 5. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏£‡∏±‡∏ô 3d-scan.py ...\")\n",
                "print(\"-\" * 50)\n",
                "\n",
                "if RESUME_PATH:\n",
                "    print(f\"üîÑ Mode: RESUME from {RESUME_PATH}\")\n",
                "    if not os.path.exists(RESUME_PATH):\n",
                "        print(f\"‚ùå Error: Resume Path not found: {RESUME_PATH}\")\n",
                "        print(\"üëâ Please check if you added the Dataset correctly on the right sidebar.\")\n",
                "    else:\n",
                "        !python 3d-scan.py --resume_path \"{RESUME_PATH}\" 2>&1 | tee 3d-scan.log.txt\n",
                "else:\n",
                "    print(f\"‚ñ∂Ô∏è Mode: NEW RUN\")\n",
                "    !python 3d-scan.py 2>&1 | tee 3d-scan.log.txt"
            ]
        }
    ],
    "metadata": {
        "kaggle": {
            "accelerator": "nvidiaTeslaT4",
            "dataSources": [],
            "dockerImageVersionId": 31259,
            "isGpuEnabled": true,
            "isInternetEnabled": true,
            "language": "python",
            "sourceType": "notebook"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}